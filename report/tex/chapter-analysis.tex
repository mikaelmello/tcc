A área de aprendizagem de máquina é extensa e diversa, modelos são em sua maioria desenvolvidos de forma personalizada para um problema único que buscam resolver. Esta diversidade é refletida na atual literatura sobre o uso de ML em EONs. Pesquisas como \cite{eon_ml_survey_2020} e \cite{8527529} mostram as diversas aplicações em diferentes problemas de EONs, apesar da pesquisa na área estar em sua infância.

Ao propor soluções que busquem auxiliar pesquisadores no uso de ML em simuladores, é necessário considerar os diversos casos de uso e suas características, fundamentais para definir a importância de fatores como desempenho do tempo de execução, facilidade de execução, disponibilidade da GPU, entre outros. Para isto, este trabalho focará suas contribuições em duas categorias, propostas de integração focadas em flexibilidade e facilidade de uso do usuário e propostas de integração focadas em um melhor desempenho na execução dos modelos.

\section{Flexibilidade}
\label{flexible}

No campo de aprendizagem de máquina, é comum que haja um constante refinamento de modelos em desenvolvimento, havendo várias iterações de testes e modificações de parâmetros de modo a maximizar o desempenho do modelo, avaliado em métricas apropriadas. Nestes casos, uma integração em que o pesquisador precisa constantemente exportar o modelo para carregá-lo em um simulador acaba se tornando trabalhosa. Além de exportar o modelo, ainda seria necessário modificar o simulador, possivelmente desenvolvido em uma linguagem não dominada pelo pesquisador, para que colete métricas relevantes ao processo de desenvolvimento do modelo.

Assim, a implementação de interfaces genéricas de comunicação em simuladores é uma boa alternativa para pesquisadores que desejam manter o controle sobre o ambiente de desenvolvimento e execução de seus modelos de aprendizagem de máquina e ainda assim integrá-los facilmente com simuladores.

Nesta solução, o simulador é programado para executar um modelo por meio de uma interface de comunicação pré-definida, como por exemplo chamadas em HTTP para um servidor configurado, e o pesquisador é responsável por implementar a interface no sistema que executa o modelo, neste exemplo um servidor HTTP. Assim, o pesquisador mantém o controle sobre o ambiente deste modelo, sendo capaz de coletar quaisquer métricas relevantes de execução como os valores de entrada e saída, tempo de execução, entre outros, e sem precisar constantemente exportar o modelo e carregá-lo no simulador a cada rodada de testes.

Sua principal desvantagem consiste em maiores tempos de execução do modelo, do ponto de vista do simulador. Isto acontece pela sobrecarga de operações para transferir os dados de entrada e saída do modelo entre processos além da latência presente na comunicação entre sistemas. Esta sobrecarga adicional de tempo de execução pode ser significativa ou não, dependendo de vários fatores acerca do estudo sendo realizado e do modelo sendo desenvolvido.

No ONS, este objetivo foi realizado por meio da implementação da interface \texttt{Model} e da classe \texttt{HttpRemoteModel}.

A interface \texttt{Model}, exibida no Apêndice \ref{appendix-ons-model}, é uma definição genérica adaptável para quaisquer tipos de entradas e saídas de um modelo, definindo a existência de um método específico para execução do modelo.

A classe \texttt{HttpRemoteModel} implementa a interface \texttt{Model}. Esta classe ao ser instanciada com uma URL, implementa o método de execução do modelo de modo que a execução do modelo é despachada para um sistema externo por meio de uma requisição HTTP POST para a URL definida. Esta requisição envia em seu corpo a entrada do modelo, serializada em JSON, e deve receber como resposta um valor do tipo de saída, também serializado em JSON.

Como prova de conceito, um simples servidor HTTP foi desenvolvido para servir a execução de modelos em Python com a biblioteca ONNX Runtime, chamado por uma instância da classe \texttt{HttpRemoteModel} apropriadamente configurada com a URL local. Ao receber uma requisição, o servidor HTTP envia os dados de entrada para o modelo utilizado para medições no Capítulo \ref{chapter-deploy}. O resultado da execução é enviado como resposta de volta ao ONS, que pode então tratar a classificação de acordo com seus objetivos, para o teste os resultados foram apenas imprimidos na tela. As implementações podem ser vistas nos Apêndices \ref{appendix-ons-remote-model} e \ref{appendix-ons-http-server}.

Com este tipo de integração, o trabalho para utilizar diferentes modelos no ONS é reduzido a instanciar uma classe e realizar a chamada de execução quando apropriado. Isto permite que o pesquisador concentre seus esforços em ambientes que já domina devido à experiência obtida durante seus estudos acerca do desenvolvimento do modelos de aprendizagem de máquina.

\section{Desempenho}

A solução com foco em desempenho busca diminuir o tempo total de execução de modelos, o que torna importante que os modelos possam ser executados de forma nativa pelo simulador uma vez que isto elimina a sobrecarga de tempo por comunicação externa.

Para simuladores desenvolvidos em Java, vimos no Capítulo \ref{chapter-deploy} que para o modelo selecionado, executá-lo com a biblioteca Deeplearning4j sem o uso de GPU exibiu o melhor desempenho. A biblioteca ONNX Runtime, apesar de mais lenta, também é atrativa por permitir a importação de modelos no formato ONNX, um formato popular e capaz de ser o alvo de conversão de diversos outros formatos existentes na literatura.

Em Python, a biblioteca ONNX Runtime com o uso de GPU exibiu um desempenho tão bom quanto OpenCV sem o uso de GPU. Ambas combinações possuem suas vantagens e desvantagens em relação a facilidade e flexibilidade no uso: OpenCV é voltada para uso na área de visão computacional, o que torna o uso dela em outros contextos sub-óptimo se ela não for a melhor escolha no quesito desempenho; ONNX Runtime precisa do uso da GPU para ter um desempenho tão bom quanto OpenCV com este modelo, porém tem suporte para fácil configuração de uso da GPU e uma API mais amigável.

Foram feitas contribuições ao ONS para permitir o carregamento e execução nativa de modelos de aprendizagem de máquina durante a execução de uma simulação no ONS. Assim, pesquisadores podem realizar implementações personalizadas de seus modelos utilizando tanto a biblioteca ONNX Runtime como a biblioteca Deeplearning4j para carregamento e execução dos modelos.

A estrutura utilizada para implementação da funcionalidade nativa tem como base a classe abstrata \texttt{NativeModel}, que implementa os métodos da interface \texttt{Model} com as seguintes adições: quatro métodos abstratos diferentes responsáveis por carregar o modelo a partir de diferentes formatos de entrada, como arquivos ou \textit{streams}; definição do método abstrato \texttt{\_predict}, que deve ser implementado de acordo com as especificações de cada modelo; implementação do método \texttt{predict} da interface \texttt{Model} que verifica se o modelo já foi carregado com sucesso e então realizada a chamada para \texttt{\_predict}.

Para adicionar o suporte ao uso de ambos Deeplearning4j e ONNX Runtime, foram implementadas duas classes, \texttt{Dl4jNativeModel} e \texttt{OnnxNativeModel}. Estas classes são responsáveis apenas por implementar os métodos de carregamento dos modelos, levando em conta as especificações de cada biblioteca e os tipos de serialização suportados.

Como prova de conceito, o modelo classificador utilizado no Capítulo \ref{chapter-deploy} foi implementado em Java para o ONS utilizando ambas bibliotecas previamente mencionadas, sob as classes \texttt{Dl4jClassifier} e \texttt{OnnxClassifier}. Estas classes extendem \texttt{Dl4jNativeModel} e \texttt{OnnxNativeModel}, respectivamente, e precisam apenas implementar o método de execução responsável por receber a entrada, executar o modelo e retornar a saída.

Assim como no teste da classe \texttt{HttpRemoteModel}, estes modelos foram integrados a uma simulação do ONS e executados em cada requisição de rede, tendo como entrada o estado atual da rede. Os resultados foram então impressos para a validação manual dos resultados e da integração bem-sucedidos. As implementações das classes relacionadas a execuções nativas de modelos podem ser encontradas no Apêndice \ref{appendix-ons-native-model}.

Com esta abordagem, pesquisadores podem tomar proveito de modelos pré-treinados e usá-los em seus algoritmos em desenvolvimento, possuindo diversas vantagens como bom desempenho e nenhuma necessidade de dependências externas.

Com estas contribuições, espera-se que atuais usuários do ONS possam utilizar modelos de ML pré-treinados em suas simulações, e que pesquisadores de ML na área de EONs sintam-se incentivados a utilizar o ONS para auxiliar-los em suas pesquisas.
