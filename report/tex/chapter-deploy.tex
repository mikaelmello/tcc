Este capítulo tem como objetivo apresentar a metodologia da avaliação de desempenho das diferentes bibliotecas de ML selecionadas.

\section{Motivação}%
\label{3-motivation}

Avaliações são importantes na busca pelo máximo desempenho de um sistema com os recursos disponíveis. Seus resultados auxiliam tanto nas decisões de escolhas entre diferentes sistemas ou simplesmente entender o funcionamento de um sistema já existente. Devido à grande diversidade de sistemas, não existe um procedimento padrão comum em que seja possível analisar eficientemente um sistema qualquer, sendo necessário conhecer o sistema a ser avaliado e escolher as métricas, carga de trabalho e técnicas de avaliação apropriadas. \cite{jain1991art}

Uma simulação executada por simuladores de EONs costuma envolver dezenas de milhares de eventos como requisições de conexões. Tamanha magnitude do número de eventos representa a importância de um bom desempenho na execução de modelos, como por exemplo em propostas de soluções para problemas de alocação de recursos (e.g. RSA) cujos modelos seriam executados em cada evento.

Assim, uma análise quantitativa do desempenho de bibliotecas de ML é importante na busca por uma solução de integração de ML com simuladores que seja flexível de acordo com as necessidades de cada pesquisa e que possua um bom desempenho de modo a acelerar a obtenção de resultados.

\section{Metodologia}%
\label{3-methodology}

De acordo com Raj Jain \cite{jain1991art}, há três métodos de avaliação de desempenho: modelagem analítica, simulação e medição. O sistema a ser avaliado, devido à presença de modelos de redes neurais profundas, é complexo o suficiente para tornar a modelagem analítica inviável. A medição foi descartada pelo fato de não buscar-se uma solução para apenas um simulador e haver um alto custo de implementação para cada possibilidade de integração. Por estes motivos, os resultados serão obtidos por meio de simulações.

\subsection{Ambiente de Simulação}

Para a realização da simulação, 8 programas foram desenvolvidos considerando todas as combinações de linguagens (Java e Python), bibliotecas (ONNX Runtime, Tensorflow Lite, OpenCV e DeepLearning4j) e o uso ou não da GPU para execução de modelos. Estes programas realizam o mesmo conjunto de tarefas: 1. carregar o modelo e inicializar procedimentos necessários para futuras execuções; 2. carregar a carga de trabalho usada como entrada do modelo; 3. executar o modelo com as entradas carregadas.

Cada programa trata de medir apenas o intervalo de tempo em que a execução do modelo ocorre, sem considerar outros fatores como o tempo de carregamento da carga de trabalho ou do modelo em si. A execução de cada instância de simulação foi orquestrada por \textit{scripts} auxiliares feitos em Python, responsáveis por instalar dependências, compilar e executar os programas de simulação.

As simulações foram realizadas em uma máquina com processador Intel Core i3-8100, placa de vídeo GeForce RTX 2060 e memória RAM de 32 GB (2x16GB 3000Mhz DDR4). O código-fonte dos programas de simulação e de programas auxiliares pode ser encontrado na url \url{tcc.mikaelmello.com}.

\subsection{Modelos e Cargas de Trabalho}

Para a simulação, um classificador baseado em \textit{deep learning} para identificação de estratégias de RSA em EONs, desenvolvido por Guilherme et. al \cite{eon_ml_classifier_2020}, será utilizado para comparação.

Este modelo recebe como entrada o estado de uma EON e tem como saída a classificação da estratégia RSA em utilização, de acordo com o estado. O modelo pode retornar 7 diferentes estratégias como saída: \textit{First-Fit}, \textit{Last-Fit}, \textit{Random-Fit}, \textit{Exact-Fit}, \textit{Best-Fit}, \textit{Pseudo Partitioning}, \textit{Dedicated Partition} e \textit{FirstLastExactFit}.

A carga de trabalho consiste de 29991 diferentes estados de rede, sendo cada estado representado por uma matriz de 86 linhas e 320 colunas, a representação da topologia USANet com 24 nós e 86 enlaces em que cada enlace contém 320 \textit{slots}.

\subsection{Métricas}

Devido à necessidade de diminuir o tempo de inferência dos modelos como elencado na seção \ref{3-motivation}, esta será a métrica de desempenho avaliada, sendo definida como o tempo total percorrido desde a chamada do serviço de execução de modelos, o método \texttt{classify} da classe \texttt{Classifier} na implementação das simulações, até o retorno da chamada.

Além disso, como a execução dos modelos em diferentes bibliotecas exigem algumas conversões do formato de serialização, será avaliado se em alguma das simulações, o resultado de execuções do modelo difere de outras simulações.

\subsection{Execução}

Foram testadas 8 combinações de linguagens, bibliotecas e uso ou não de GPU:

\begin{itemize}
  \item Java, DeepLearning4j, sem GPU;
  \item Java, DeepLearning4j, com GPU;
  \item Java, ONNX Runtime, sem GPU;
  \item Java, ONNX Runtime, com GPU;
  \item Python, OpenCV, sem GPU;
  \item Python, Tensorflow Lite, sem GPU;
  \item Python, ONNX Runtime, sem GPU;
  \item Python, ONNX Runtime, com GPU;
\end{itemize}

Cada uma destas combinações, aqui chamada de simulação, foi realizada cinco vezes apresentando intervalos de confiança 95\% de confiabilidade.